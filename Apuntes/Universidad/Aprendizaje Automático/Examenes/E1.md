
# Ej 1
Describa con precisiÃ³n el contenido yÂ  la funciÃ³n de cada uno de los elementos de un problema de aprendizajeÂ  Â $X,Y,f$Â  para los tres ejemplos dados.Â  Diga que problema de aprendizaje representa cada uno de ellos, Justifique su razonamiento con precisiÃ³n.

a) Asignar animales a su clase o especie a partir de fotos.

b) Establecer un diagnostico de presencia caries a partir de radiografÃ­as bucales.

c) Establecer perfiles de clientes, en una empresa, a partir de su datos de compra.

### Respuesta

>A:
>> 


### SoluciÃ³n
a)Â No es un problema de aprendizaje. La clase o especie de un animal no estÃ¡ definida por sus caracteristicas de apariencia en un foto. No se puede resolver por aprendizaje. La funciÃ³n f no existe.

b)Â  Problema de clasificaciÃ³n supervisada,. X es la radiografia, Y=[+1,-1] y f la funciÃ³n que mapea cada X en Y.

c) Es un problema de aprendizaje, principalmente no-supervisado ya que no hay etiquetas. Los datos X={tikets de compra} se pueden agrupar en perfiles usando algoritmos de agrupamiento. No hay Y.





# Ej 2
Considere el problema de aprendizaje estadÃ­stico por inducciÃ³n de clasificaciÃ³n binaria yÂ  diga:Â Â¿Tiene sentido buscar el mejor clasificador posible?Â 

a) En caso negativo, diga por quÃ©Â  y cuales serÃ­an las alternativas de actuaciÃ³n.

b) En caso positivo argumente los resultados que lo avalan.

### Respuesta:

> No tiene sentido buscar el mejor clasificador posible. El teorema de No Free Lunch (NFL) establece todos los clasificadores son iguales en promedio para todas las distribuciones posibles. Esto hace que sea mejor aplicar otro tipo de criterios que nos aseguren un buen error a lo largo del tiempo.
> Las alternativas pasan por reducir la dimensiÃ³n VC con el objetivo de enocntrar una soluciÃ³n PAC

### SoluciÃ³n
El teorema de Non-Free Lunch nos dice que que todos los clasificadores son equivalentes en media sobre todas las posibles distribuciones, por lo que no tiene sentido buscar el mejor clasificador definido sobre todas las posibles funciones en [0,1].

Las alternativas que nos deja este resultado esÂ  restringuir la clases de funciones con la que podamos aprender. Para ello se introducen restricciones oÂ  sesgo inductivo sobre la clase de funciones, lo que nos permite encontrar soluciones PAC sobre ellas. La dimensiÃ­on de VC finita es laÂ  condiciÃ³n/restricciÃ³n mÃ¡s general enocntrada hasta la fecha,

# Ej 3

a) Identifique el origen de las fuentes de incertidumbre presentes en la soluciÃ³n PAC de un problema de aprendizaje. Justifique la respuesta.

b) CÃ³mo afecta la clase de funcionesÂ  elegida al error de una soluciÃ³n PAC?Â Â Justifique la respuesta

### Respuesta

> A:
>> La incertidumbre de la soluciÃ³n PAC pasa por la dependencia de varios resultados estadÃ­sticos a la hora de desarrollar su demostraciÃ³n. La primera serÃ­a el suponer que los datos de entrenamiento han sido idÃ©nticamente e independientemente distribuidas (i.i.d) respecto a una distribuciÃ³n de probabilidad $D$. -> Esto hace que varios dataset del mismo problema devuelvan predictores distintos. 
>> La otra fuente de incertidumbre serÃ­a la estimaciÃ³n que se hace para calcular el error verdadero (fuera de la muestra) a partir del error generado dentro de la muestra obtenida.
>
>B:
>> LA caracterÃ­stica clave del conjunto hipÃ³tesis que influye de manera directa en una soluciÃ³n PAC es la complegidad de la propia clase de funciones. Cuanto mas compleja sea esta clase, mas hipÃ³tesis puede generar y adaptarse pero mas probabilidades hay de que se sobreajuste y aprenda casos extremos que no mejoran la predicciÃ³n, sino que la empeoran.


### SoluciÃ³n
a)Â  La soluciÃ³n PAC tiene dos fuentes de incertidumbre provenientes de las muestra.Â 

1) La primera estÃ¡ asociada al error entre la estimaciÃ³n de la muestra y el verdadero valor en la poblaciÃ³n y tiene como origen que el tamaÃ±o de la muestra necesariamente siempre es menor que el tamaÃ±o de la poblaciÃ³n.

2) La segunda fuente de incertidumbre esta asociada a que la muestra se elige de forma aleatoria y por tanto para distintas muestras del mismo tamaÃ±o no podemos garantizar el mismo resultado pero si una probabildad sobre el resultado.

b) En la soluciÃ³n PAC tenemos que minimizar dos errores a la vez:Â ğ¸ğ‘–ğ‘›ï¿½ï¿½ï¿½Â yÂ ğ¸ğ‘œğ‘¢ğ‘¡âˆ’ğ¸ğ‘–ğ‘›ï¿½ï¿½ï¿½ï¿½âˆ’ï¿½ï¿½ï¿½. Supongamos un tamaÃ±o muestralÂ  N fijo y suficiente. En este caso, estos dos errores solo dependen de la clase de funciones. Con clases de funciones deÂ Â ğ‘‘ğ‘£ğ‘ï¿½ï¿½ï¿½Â pequeÃ±a, serÃ¡ difÃ­cil, en general,Â  hacer queÂ ğ¸ğ‘–ğ‘›â†’0ï¿½ï¿½ï¿½â†’0Â , pero si podremos hacerÂ ğ¸ğ‘œğ‘¢ğ‘¡âˆ’ğ¸ğ‘–ğ‘›â†’0ï¿½ï¿½ï¿½ï¿½âˆ’ï¿½ï¿½ï¿½â†’0Â si N es suficientemente grande. En cambio si laÂ Â ğ‘‘ğ‘£ğ‘ï¿½ï¿½ï¿½Â es grande , serÃ¡ fÃ¡cilÂ  hacer queÂ ğ¸ğ‘–ğ‘›â†’0ï¿½ï¿½ï¿½â†’0Â , pero serÃ¡ difÃ­cil queÂ Â ğ¸ğ‘œğ‘¢ğ‘¡âˆ’ğ¸ğ‘–ğ‘›â†’0ï¿½ï¿½ï¿½ï¿½âˆ’ï¿½ï¿½ï¿½â†’0. El tamaÃ±o muestral N fija la complejidad mÃ¡xima de la clase para un error dado..Â  En consecuencia,Â  la elecciÃ³n de la clase de funciones es clave para obtener una buena soluciÃ³n PAC y la potencia de la misma debeÂ  depender del tamaÃ±o muestral disponible.

# Ej 4
Considere la funciÃ³n de errorÂ  definida para una muestraÂ $(\vec{x},y)$ porÂ $E({\bf w})=( max(0,1- y {\bf w}^T{\bf x}))^2$Â ,Â Â dondeÂ $\vec{x}$Â yÂ $\vec{w}$Â son vectores de igual dimensiÃ³n fijaÂ  eÂ $y \in \{âˆ’1,1\}$Â ,

a) Deducir la regla de adaptaciÃ³n de gradiente descendente para los componentes deÂ Â ğ°ï¿½.Â Â 

b) Considere ahora una muestra de tamaÃ±o N, Â $({\bf x_n},y_n), n=1,\cdots,N$ . Usar el resultado anterior para deducir la regla de adaptaciÃ³n para la funciÃ³n de errorÂ Â $\frac{1}{N} \sum_{n=1}^N E_n({\bf w})$Â  asociada al promedio de las n muestras.





### SoluciÃ³n
5
a) DerivandoÂ $E(\bf w )$respecto deÂ $\bf w$, tenemosÂ $\frac{\partial E_{\bf w}}{\partial {\bf w}} = -2y {\bf x} \cdot max(0, -y {\bf w}^T{\bf x})$. Entonces la regla de adaptaciÃ³n del GD paraÂ Â $\bf w$Â  Â esÂ ${\bf w}_{new}={\bf w}_{old}+\lambda 2{\bf x}\cdot max(0,y-{\bf w}^T_{old}{\bf x})$. Es decir,${\bf w}_{new}={\bf w}_{old}$Â siÂ y ${\bf w}^T_{old}{\bf x} \geq1$ y ${\bf w}_{new}={\bf w}_{old}+\lambda 2{\bf x}(y-{\bf w}^T_{old}{\bf x})$ Â en caso contrario.


b) Ahora se pide calcular el gradiente de un promedio que se calcula como el promedio de los gradientes individuales,Â 
Â ${\bf w}_{new} = {\bf w}_{old}+\lambda \frac{2}{N} \sum_{i=1}^NÂ  {\bf x}_i \cdot max(0, y_i-{\bf w_{old}}^T{\bf x}_i)$
Â 
Â O tambiÃ©nÂ ${\bf w}_{new} = {\bf w}_{old}+\frac{2\lambda}{N} \sum_{j=1}^N {\bf x}_j(y_j-{\bf w_{old}}^T{\bf x_j})$Â paraÂ $y_j > {\bf w_{old}}^T{\bf x_j}$ 
Â 


# Ej 5

CÃºales de las siguientes expresiones son posibles funciones de crecimientoÂ $m_{\cal H}(N)$ Â para algÃºn conjunto de hipÃ³tesis

Â $1+N\,\;;\; 1+N+\frac{N(N-1)}{2}\,\;; \,\; 2^N\,\;; \,\; 2^{\lfloor{\sqrt{N}}\rfloor}\,\;; \,\; 2^{\lfloor{N/2}\rfloor}\,\;; \,\; 1+N+\frac{N(N-1)(N-2)}{6}$ 

dondeÂ $\lfloor\cdot\rfloor$Â significa parte entera. Decir por quÃ©.

J
### SoluciÃ³n
$2^N$Â  y todas todas aquellas que son polinÃ³micas en N de acuerdo al resultadÂ  deÂ Sauer-Shelah-Perles: 1,2,3,6



# Ej6
a) DigaÂ  como se define el error de BayesÂ  y que papel juegaÂ  en la teorÃ­a del aprendizaje estadÃ­stico.

b) Diga si el error de Bayes es un elemento de ayuda en el desarrollo de nuevos algoritmos de aprendizaje automÃ¡tico.


### SoluciÃ³n
a) Ahora se supone que conocemos la distribuciÃ³n de probabilidad sobre el espacio X xY. El errror de Bayes se define como el error que se obtieneÂ  de clasificar toda la poblaciÃ­on con la regla: Asignar cada item a su clase de mayor probabilidad. Dado que se supone el conocimeneto de la distribuciÃ³n de probabilidad sobre la poblaciÃ³n, este error defineÂ  la cota inferior del error que es posible alcanzar en un determindao problema con cualquier clasificador.

  

b)Â  No lo es, ya queÂ  en el desarrollo de clasificadores se minimizan funciones de la muestra que no tienen conocimiento de la distribuciÃ³n de la poblaciÃ³n. Por otro lado, el hecho de que represente el menor error posible no es una ventaja en el desarrollo de nuevos algoritmos.


# Ej 7

Â La dimensiÃ³n de Vapnik-Chervonenkis (VC) es un valor numÃ©rico asociado aÂ  uno deÂ  los elementos que definen el problema del aprendizaje estadÃ­stico.Â¿Que nos dice dicha propiedad y a que elemento del aprendizaje se refiere? Detallar la contestaciÃ³n.  

> Indica la cantidad mÃ­nima de puntos que un conjunto de hipÃ³tesis no es capaz de separar de manera unÃ­voca. $1-d_{cv}$ es un punto de ruptura del conjunto hipÃ³tesis. Que se define como el nÃºmero mÃ¡ximo de dicotomias que pueden separar las funciones pertenecientes al conjunto hipÃ³tesis. 
> De manera abstracta mide la complejidad del conjunto hipÃ³tesis y es un elemento fundamental en el estudio de si el modelo es capaz de generalizar fuera de la muestra o no.


### SoluciÃ³n
La dimensiÃ³n VC es una propiedad de la clase de funciones usada y (en clasificaciÃ³n binaria) nos da el mayor tamaÃ±o muestral para el cual la clase de funciones es capaz de explicar completamente todas sus dicotomias.Â 

El caso de dimensiÃ³n-VC finita nos garantiza que la regla ERM es suficiente para lograr una soluciÃ³n PAC al aprendizaje inductivo. En el casoÂ  de dimensiÃ³n-VC infinita,Â  solo se puede garantizar una soluciÃ³nÂ bajo determinadas condiciones sobre la clase de funciones (jerarquia de clases de dimensiÃ³n VC finita).


# Ej 8
Considere que trabaja en una empresa dedicada a la construcciÃ³n de mÃ¡quinas para la clasificaciÃ³nÂ  automÃ¡tica de piezas de fruta por tipos y variedades. En particular la empresa trabaja con todo tipo de frutas duras no arracimadas como naranjas, manzanas, peras, melocotÃ³n, mangos, et>c., hasta un total de 50 tipos distintos y cuatro variedades como mÃ¡ximo por tipo. Analice la situaciÃ³n y diga:

A) Â¿Considera que este problema es un problema para ser resuelto por aprendizaje o por diseÃ±o? Justifique su decisiÃ³n con argumentos.

B) En el caso de que decidanÂ  aproximarlo con aprendizaje:
	B1) Establezca de forma clara y precisa la estructura del espacio de etiquetasÂ _Y_Â y la funciÃ³nÂ _f_Â que etiqueta las piezas.Ãº
	B2) Un experto le indica que entre otras informaciones las siguientes variables deberÃ­an ser consideradas: color: (RGB), grado de esfericidad:, un valor en (0,1], volumen: un valor enÂ cm^3, tipo de de piel:Â  un valor del conjunto (0,1,2).Â 

Describa la longitud y contenido del vector (**x**,y), siendoÂ **x**Â la codificaciÃ³n dada al computador de las variables enumeradasÂ  e y su etiqueta.



>A:
>> Para ser resuelto por diseÃ±o se requerirÃ­a un modelo computacional y matemÃ¡tico que relacione las caracterÃ­sticas con las etiquetas. Aparenta ser un problema relativamente sencillo de resolver con tÃ©cnicas de aprendizaje automÃ¡tico. 
>
>B:
>>Las etiquetas ($Y$) se podrÃ­an representar como un vector de 1 y -1 con una longitud de: $50*4=200$, en el que la posiciÃ³n que representa el tipo de fruta valga 1 y el resto -1. la funciÃ³n f es una funciÃ³n que relaciona el espacio de los datos (codificaciÃ³n de los datos del dataset) con el espacio de las etiquetas.
>>El rgb se codifica con 3 valores enteros, la escericidad con un valor real entre 0 y 1, el volumen es un valor real y el tipo de piel con un 1 en la posiciÃ³n correspondiente a cada tipo y un -1 en las otras dos. junto a las 200 dimensiones del espacio de las etiquetas se queda: $200+3+1+1+3=208$. El dataset lo compondrÃ­an vectores de 208 caracterÃ­sticas cada una con esta distribuciÃ³n:  //RGB*3//esfericidad//Vol//Tipo_Piel*3//Etiqueta//



### SoluciÃ³n
A) Es un problema de aprendizaje. DiseÃ±o aunque posible, necesitarÃ­a delÂ conocimientoÂ de un modelo computacional sobre las caracteristicas de las frutas y susÂ  variedades, y hasta la fecha no existen dichos modelos .Â  Si se aprende los modelos entonces serÃ­a aprendizaje.

50 tipos x 4 variedades son 200 clases.Â 

B1) Y esta compuesto por 200 etiquetas definidasÂ  cada una por un vector un binario de 200 elementos. Todos los elementos serÃ­an -1 menos uno, con un 1, en la posiciÃ³n ordinal de la clase.Â  f es la funciÃ³n mapea cada vector de caraterÃ­sticas en un elemento de Y.

B2)Â  Cada variable debe expresarse como un valor nÃºmerico. Por tanto color es un vector de tres nÃºmeros reales, esfericidad y volumen un nÃºmero cada uno y piel (categÃ³rica) un vector de tres elementos binarios (0 o 1). En total x se codifica en un vector de 3+1+1+3= 8 nÃºmeros e y en vector de 200 valores. Por tanto (x,y) es un vector de 208 nÃºmeros.


# Ej 9
Cuando calculamos la soluciÃ³n PACÂ  de un problema de aprendizaje Â¿es posible encontrar situaciones en las que se puede decir:Â  " hemos fallado en encontrar una soluciÃ³n" ?.Â Â 

b) En caso afirmativo, decir cuando se produce dicho evento, bajo que circunstancias se produceÂ  y como sabemos que hemos fallado.

c) En caso negativo, justificar por que no cabe decir esa expresiÃ³n.

Justificar con argumentos tÃ©cnico

### SoluciÃ³n
Si se puede dar la situaciÃ³n de fallo. Cuando el Error de ajusteÂ E_{in}Â sea grande. Esto dependerÃ¡ de la clase de funciones elegida.

La desigualdadÂ $\mathbb P[|E_{out}-E_{in}|>\epsilon(N,d_{vc})] \leq \delta(N, d_{vc})$ Â nos sirve para analizar cuando se produce. Los valoresÂ  de las constantesÂ $\epsilon$Â yÂ $\delta$Â dependen del tamaÃ±o muestral y de la clase de funciones.Â 

El objetivo es dar una soluciÃ³n con la que haya alguna posibilidad de que el error fuera de la muestra sea cero o este tan cerca de cero como lo permita la muestra N. Para ello, y de acuerdo a la expresiÃ³n anterior, debe verificarse queÂ $E_{in}\approx 0$, en caso contrario el errorÂ $E_{out}$Â no serÃ¡ pequeÃ±o en funciÃ³n deÂ $\epsilon(N,d_{vc})$.Â  En consecuencia si no logramos realizar un ajuste que verifiqueÂ $E_{in}\approx 0$Â para una clase de funciones de baja complejidad diremos que hemos fallado con dicha clase de funciones.


# Ej 10
Determinar el "punto de ruptura" de la clase de funcionesÂ  que etiqueta la recta realÂ  a partir deÂ  subconjuntos de la recta real formada por la uniÃ³n de k intervalos finitos.Â  Es decir, para cadaÂ  k intervalos finitos fijados, un punto dentro de su uniÃ³n tiene una etiqueta distinta a un punto fuera de dicho subconjunto..

### SoluciÃ³n
Consideremos el caso que representa la situaciÃ³n mÃ¡s general de puntos etiquetados en una recta, es decir un modelo alternado de etiquetas, ......x,o,x,o,x,o,x,.....Â 

Si analizamos el casoÂ Â k=1 vemos que elÂ punto de ruptura es 3 ya que solo podemos separar dos par
> De manera abstracta mide la complejidad del conjunto hipÃ³tesis y es un elemento fundamental en ticiones, los casos (x o, o x) . k=2Â el punto de ruptura es 5 ( oxoxo , xoxox).Â k=3 el punto de ruptura es 7, etc. Entonces por inducciÃ³n 2k+1


# Ej 11
a) Establezca el papel que juega la regla ERMÂ  en la soluciÃ³n PAC de un problema de aprendizaje estadÃ­stico. Justifiquela.

b) Analice la regla SRM de aprendizaje y diga de que forma generaliza a la soluciÃ³n PAC.

### SoluciÃ³n
1.- La regla ERM garantiza una soluciÃ³n PAC al aprendizaje por inducciÃ³n sobreÂ  clases de funciones de dimensiÃ³n VC finita. Por tanto garantiza la extrapolaciÃ­on de los resultados dentro de la muestra a fuera de la muestra.Â 

2.-Â Â La regla SRMÂ  funciona sobre una jerarquÃ­a de clases de dimensiÃ³n VC finita. Debe resolver un problema PAC por cada clase de funciones de la jerarquÃ­a. Por tanto los tamaÃ±os muestrales mÃ­nimos para garantizar la soluciÃ³n PAC en cada clase de la jerarquÃ­a no tienen que ser iguales. En consecuenciaÂ  la soluciÃ³n SRM no es una soluciÃ³n PACÂ  sobre la clase de funciones conÂ $d_{vc}=\infty$.Â  Â LaÂ generaliza en ese sentido, no hay un N a partir del cual podamos garantizar un error prefijado. El tamaÃ±o de N depende de la clase de funciones.