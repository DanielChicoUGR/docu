
# Ej 1
Describa con precisión el contenido y  la función de cada uno de los elementos de un problema de aprendizaje   $X,Y,f$  para los tres ejemplos dados.  Diga que problema de aprendizaje representa cada uno de ellos, Justifique su razonamiento con precisión.

a) Asignar animales a su clase o especie a partir de fotos.

b) Establecer un diagnostico de presencia caries a partir de radiografías bucales.

c) Establecer perfiles de clientes, en una empresa, a partir de su datos de compra.

### Respuesta

>A:
>> 


### Solución
a) No es un problema de aprendizaje. La clase o especie de un animal no está definida por sus caracteristicas de apariencia en un foto. No se puede resolver por aprendizaje. La función f no existe.

b)  Problema de clasificación supervisada,. X es la radiografia, Y=[+1,-1] y f la función que mapea cada X en Y.

c) Es un problema de aprendizaje, principalmente no-supervisado ya que no hay etiquetas. Los datos X={tikets de compra} se pueden agrupar en perfiles usando algoritmos de agrupamiento. No hay Y.





# Ej 2
Considere el problema de aprendizaje estadístico por inducción de clasificación binaria y  diga: ¿Tiene sentido buscar el mejor clasificador posible? 

a) En caso negativo, diga por qué  y cuales serían las alternativas de actuación.

b) En caso positivo argumente los resultados que lo avalan.

### Respuesta:

> No tiene sentido buscar el mejor clasificador posible. El teorema de No Free Lunch (NFL) establece todos los clasificadores son iguales en promedio para todas las distribuciones posibles. Esto hace que sea mejor aplicar otro tipo de criterios que nos aseguren un buen error a lo largo del tiempo.
> Las alternativas pasan por reducir la dimensión VC con el objetivo de enocntrar una solución PAC

### Solución
El teorema de Non-Free Lunch nos dice que que todos los clasificadores son equivalentes en media sobre todas las posibles distribuciones, por lo que no tiene sentido buscar el mejor clasificador definido sobre todas las posibles funciones en [0,1].

Las alternativas que nos deja este resultado es  restringuir la clases de funciones con la que podamos aprender. Para ello se introducen restricciones o  sesgo inductivo sobre la clase de funciones, lo que nos permite encontrar soluciones PAC sobre ellas. La dimensiíon de VC finita es la  condición/restricción más general enocntrada hasta la fecha,

# Ej 3

a) Identifique el origen de las fuentes de incertidumbre presentes en la solución PAC de un problema de aprendizaje. Justifique la respuesta.

b) Cómo afecta la clase de funciones  elegida al error de una solución PAC?  Justifique la respuesta

### Respuesta

> A:
>> La incertidumbre de la solución PAC pasa por la dependencia de varios resultados estadísticos a la hora de desarrollar su demostración. La primera sería el suponer que los datos de entrenamiento han sido idénticamente e independientemente distribuidas (i.i.d) respecto a una distribución de probabilidad $D$. -> Esto hace que varios dataset del mismo problema devuelvan predictores distintos. 
>> La otra fuente de incertidumbre sería la estimación que se hace para calcular el error verdadero (fuera de la muestra) a partir del error generado dentro de la muestra obtenida.
>
>B:
>> LA característica clave del conjunto hipótesis que influye de manera directa en una solución PAC es la complegidad de la propia clase de funciones. Cuanto mas compleja sea esta clase, mas hipótesis puede generar y adaptarse pero mas probabilidades hay de que se sobreajuste y aprenda casos extremos que no mejoran la predicción, sino que la empeoran.


### Solución
a)  La solución PAC tiene dos fuentes de incertidumbre provenientes de las muestra. 

1) La primera está asociada al error entre la estimación de la muestra y el verdadero valor en la población y tiene como origen que el tamaño de la muestra necesariamente siempre es menor que el tamaño de la población.

2) La segunda fuente de incertidumbre esta asociada a que la muestra se elige de forma aleatoria y por tanto para distintas muestras del mismo tamaño no podemos garantizar el mismo resultado pero si una probabildad sobre el resultado.

b) En la solución PAC tenemos que minimizar dos errores a la vez: 𝐸𝑖𝑛��� y 𝐸𝑜𝑢𝑡−𝐸𝑖𝑛����−���. Supongamos un tamaño muestral  N fijo y suficiente. En este caso, estos dos errores solo dependen de la clase de funciones. Con clases de funciones de  𝑑𝑣𝑐��� pequeña, será difícil, en general,  hacer que 𝐸𝑖𝑛→0���→0 , pero si podremos hacer 𝐸𝑜𝑢𝑡−𝐸𝑖𝑛→0����−���→0 si N es suficientemente grande. En cambio si la  𝑑𝑣𝑐��� es grande , será fácil  hacer que 𝐸𝑖𝑛→0���→0 , pero será difícil que  𝐸𝑜𝑢𝑡−𝐸𝑖𝑛→0����−���→0. El tamaño muestral N fija la complejidad máxima de la clase para un error dado..  En consecuencia,  la elección de la clase de funciones es clave para obtener una buena solución PAC y la potencia de la misma debe  depender del tamaño muestral disponible.

# Ej 4
Considere la función de error  definida para una muestra $(\vec{x},y)$ por $E({\bf w})=( max(0,1- y {\bf w}^T{\bf x}))^2$ ,  donde $\vec{x}$ y $\vec{w}$ son vectores de igual dimensión fija  e $y \in \{−1,1\}$ ,

a) Deducir la regla de adaptación de gradiente descendente para los componentes de  𝐰�.  

b) Considere ahora una muestra de tamaño N,  $({\bf x_n},y_n), n=1,\cdots,N$ . Usar el resultado anterior para deducir la regla de adaptación para la función de error  $\frac{1}{N} \sum_{n=1}^N E_n({\bf w})$  asociada al promedio de las n muestras.





### Solución
5
a) Derivando $E(\bf w )$respecto de $\bf w$, tenemos $\frac{\partial E_{\bf w}}{\partial {\bf w}} = -2y {\bf x} \cdot max(0, -y {\bf w}^T{\bf x})$. Entonces la regla de adaptación del GD para  $\bf w$   es ${\bf w}_{new}={\bf w}_{old}+\lambda 2{\bf x}\cdot max(0,y-{\bf w}^T_{old}{\bf x})$. Es decir,${\bf w}_{new}={\bf w}_{old}$ si y ${\bf w}^T_{old}{\bf x} \geq1$ y ${\bf w}_{new}={\bf w}_{old}+\lambda 2{\bf x}(y-{\bf w}^T_{old}{\bf x})$  en caso contrario.


b) Ahora se pide calcular el gradiente de un promedio que se calcula como el promedio de los gradientes individuales, 
 ${\bf w}_{new} = {\bf w}_{old}+\lambda \frac{2}{N} \sum_{i=1}^N  {\bf x}_i \cdot max(0, y_i-{\bf w_{old}}^T{\bf x}_i)$
 
 O también ${\bf w}_{new} = {\bf w}_{old}+\frac{2\lambda}{N} \sum_{j=1}^N {\bf x}_j(y_j-{\bf w_{old}}^T{\bf x_j})$ para $y_j > {\bf w_{old}}^T{\bf x_j}$ 
 


# Ej 5

Cúales de las siguientes expresiones son posibles funciones de crecimiento $m_{\cal H}(N)$  para algún conjunto de hipótesis

 $1+N\,\;;\; 1+N+\frac{N(N-1)}{2}\,\;; \,\; 2^N\,\;; \,\; 2^{\lfloor{\sqrt{N}}\rfloor}\,\;; \,\; 2^{\lfloor{N/2}\rfloor}\,\;; \,\; 1+N+\frac{N(N-1)(N-2)}{6}$ 

donde $\lfloor\cdot\rfloor$ significa parte entera. Decir por qué.

J
### Solución
$2^N$  y todas todas aquellas que son polinómicas en N de acuerdo al resultad  de Sauer-Shelah-Perles: 1,2,3,6



# Ej6
a) Diga  como se define el error de Bayes  y que papel juega  en la teoría del aprendizaje estadístico.

b) Diga si el error de Bayes es un elemento de ayuda en el desarrollo de nuevos algoritmos de aprendizaje automático.


### Solución
a) Ahora se supone que conocemos la distribución de probabilidad sobre el espacio X xY. El errror de Bayes se define como el error que se obtiene  de clasificar toda la poblaciíon con la regla: Asignar cada item a su clase de mayor probabilidad. Dado que se supone el conocimeneto de la distribución de probabilidad sobre la población, este error define  la cota inferior del error que es posible alcanzar en un determindao problema con cualquier clasificador.

  

b)  No lo es, ya que  en el desarrollo de clasificadores se minimizan funciones de la muestra que no tienen conocimiento de la distribución de la población. Por otro lado, el hecho de que represente el menor error posible no es una ventaja en el desarrollo de nuevos algoritmos.


# Ej 7

 La dimensión de Vapnik-Chervonenkis (VC) es un valor numérico asociado a  uno de  los elementos que definen el problema del aprendizaje estadístico.¿Que nos dice dicha propiedad y a que elemento del aprendizaje se refiere? Detallar la contestación.  

> Indica la cantidad mínima de puntos que un conjunto de hipótesis no es capaz de separar de manera unívoca. $1-d_{cv}$ es un punto de ruptura del conjunto hipótesis. Que se define como el número máximo de dicotomias que pueden separar las funciones pertenecientes al conjunto hipótesis. 
> De manera abstracta mide la complejidad del conjunto hipótesis y es un elemento fundamental en el estudio de si el modelo es capaz de generalizar fuera de la muestra o no.


### Solución
La dimensión VC es una propiedad de la clase de funciones usada y (en clasificación binaria) nos da el mayor tamaño muestral para el cual la clase de funciones es capaz de explicar completamente todas sus dicotomias. 

El caso de dimensión-VC finita nos garantiza que la regla ERM es suficiente para lograr una solución PAC al aprendizaje inductivo. En el caso  de dimensión-VC infinita,  solo se puede garantizar una solución bajo determinadas condiciones sobre la clase de funciones (jerarquia de clases de dimensión VC finita).


# Ej 8
Considere que trabaja en una empresa dedicada a la construcción de máquinas para la clasificación  automática de piezas de fruta por tipos y variedades. En particular la empresa trabaja con todo tipo de frutas duras no arracimadas como naranjas, manzanas, peras, melocotón, mangos, et>c., hasta un total de 50 tipos distintos y cuatro variedades como máximo por tipo. Analice la situación y diga:

A) ¿Considera que este problema es un problema para ser resuelto por aprendizaje o por diseño? Justifique su decisión con argumentos.

B) En el caso de que decidan  aproximarlo con aprendizaje:
	B1) Establezca de forma clara y precisa la estructura del espacio de etiquetas _Y_ y la función _f_ que etiqueta las piezas.ú
	B2) Un experto le indica que entre otras informaciones las siguientes variables deberían ser consideradas: color: (RGB), grado de esfericidad:, un valor en (0,1], volumen: un valor en cm^3, tipo de de piel:  un valor del conjunto (0,1,2). 

Describa la longitud y contenido del vector (**x**,y), siendo **x** la codificación dada al computador de las variables enumeradas  e y su etiqueta.



>A:
>> Para ser resuelto por diseño se requeriría un modelo computacional y matemático que relacione las características con las etiquetas. Aparenta ser un problema relativamente sencillo de resolver con técnicas de aprendizaje automático. 
>
>B:
>>Las etiquetas ($Y$) se podrían representar como un vector de 1 y -1 con una longitud de: $50*4=200$, en el que la posición que representa el tipo de fruta valga 1 y el resto -1. la función f es una función que relaciona el espacio de los datos (codificación de los datos del dataset) con el espacio de las etiquetas.
>>El rgb se codifica con 3 valores enteros, la escericidad con un valor real entre 0 y 1, el volumen es un valor real y el tipo de piel con un 1 en la posición correspondiente a cada tipo y un -1 en las otras dos. junto a las 200 dimensiones del espacio de las etiquetas se queda: $200+3+1+1+3=208$. El dataset lo compondrían vectores de 208 características cada una con esta distribución:  //RGB*3//esfericidad//Vol//Tipo_Piel*3//Etiqueta//



### Solución
A) Es un problema de aprendizaje. Diseño aunque posible, necesitaría del conocimiento de un modelo computacional sobre las caracteristicas de las frutas y sus  variedades, y hasta la fecha no existen dichos modelos .  Si se aprende los modelos entonces sería aprendizaje.

50 tipos x 4 variedades son 200 clases. 

B1) Y esta compuesto por 200 etiquetas definidas  cada una por un vector un binario de 200 elementos. Todos los elementos serían -1 menos uno, con un 1, en la posición ordinal de la clase.  f es la función mapea cada vector de caraterísticas en un elemento de Y.

B2)  Cada variable debe expresarse como un valor númerico. Por tanto color es un vector de tres números reales, esfericidad y volumen un número cada uno y piel (categórica) un vector de tres elementos binarios (0 o 1). En total x se codifica en un vector de 3+1+1+3= 8 números e y en vector de 200 valores. Por tanto (x,y) es un vector de 208 números.


# Ej 9
Cuando calculamos la solución PAC  de un problema de aprendizaje ¿es posible encontrar situaciones en las que se puede decir:  " hemos fallado en encontrar una solución" ?.  

b) En caso afirmativo, decir cuando se produce dicho evento, bajo que circunstancias se produce  y como sabemos que hemos fallado.

c) En caso negativo, justificar por que no cabe decir esa expresión.

Justificar con argumentos técnico

### Solución
Si se puede dar la situación de fallo. Cuando el Error de ajuste E_{in} sea grande. Esto dependerá de la clase de funciones elegida.

La desigualdad $\mathbb P[|E_{out}-E_{in}|>\epsilon(N,d_{vc})] \leq \delta(N, d_{vc})$  nos sirve para analizar cuando se produce. Los valores  de las constantes $\epsilon$ y $\delta$ dependen del tamaño muestral y de la clase de funciones. 

El objetivo es dar una solución con la que haya alguna posibilidad de que el error fuera de la muestra sea cero o este tan cerca de cero como lo permita la muestra N. Para ello, y de acuerdo a la expresión anterior, debe verificarse que $E_{in}\approx 0$, en caso contrario el error $E_{out}$ no será pequeño en función de $\epsilon(N,d_{vc})$.  En consecuencia si no logramos realizar un ajuste que verifique $E_{in}\approx 0$ para una clase de funciones de baja complejidad diremos que hemos fallado con dicha clase de funciones.


# Ej 10
Determinar el "punto de ruptura" de la clase de funciones  que etiqueta la recta real  a partir de  subconjuntos de la recta real formada por la unión de k intervalos finitos.  Es decir, para cada  k intervalos finitos fijados, un punto dentro de su unión tiene una etiqueta distinta a un punto fuera de dicho subconjunto..

### Solución
Consideremos el caso que representa la situación más general de puntos etiquetados en una recta, es decir un modelo alternado de etiquetas, ......x,o,x,o,x,o,x,..... 

Si analizamos el caso  k=1 vemos que el punto de ruptura es 3 ya que solo podemos separar dos par
> De manera abstracta mide la complejidad del conjunto hipótesis y es un elemento fundamental en ticiones, los casos (x o, o x) . k=2 el punto de ruptura es 5 ( oxoxo , xoxox). k=3 el punto de ruptura es 7, etc. Entonces por inducción 2k+1


# Ej 11
a) Establezca el papel que juega la regla ERM  en la solución PAC de un problema de aprendizaje estadístico. Justifiquela.

b) Analice la regla SRM de aprendizaje y diga de que forma generaliza a la solución PAC.

### Solución
1.- La regla ERM garantiza una solución PAC al aprendizaje por inducción sobre  clases de funciones de dimensión VC finita. Por tanto garantiza la extrapolaciíon de los resultados dentro de la muestra a fuera de la muestra. 

2.-  La regla SRM  funciona sobre una jerarquía de clases de dimensión VC finita. Debe resolver un problema PAC por cada clase de funciones de la jerarquía. Por tanto los tamaños muestrales mínimos para garantizar la solución PAC en cada clase de la jerarquía no tienen que ser iguales. En consecuencia  la solución SRM no es una solución PAC  sobre la clase de funciones con $d_{vc}=\infty$.   La generaliza en ese sentido, no hay un N a partir del cual podamos garantizar un error prefijado. El tamaño de N depende de la clase de funciones.